{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/josebel78/03MIAR_Algoritmos-de-Optimizacion/blob/main/Algoritmos_Jose_Belenguer_AG3_reto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljZMs2rg93q6"
   },
   "source": [
    "# TFM_VIU_09MIAR_JBB\n",
    "## JosÃ© Belenguer Ballester\n",
    "### Enlace del repositorio GitHub:\n",
    "#### https://github.com/josebel78/TFM_VIU_09MIAR_JBB.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-gbxt9BFfdN"
   },
   "source": [
    "## MODULE IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qP-Y3Gkfc4vn"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHRiJhTnFmuV"
   },
   "source": [
    "## DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_instance(file_name):\n",
    "        \n",
    "    global J, M, Rmax\n",
    "    \n",
    "    with open(file_name,\"r\") as file:\n",
    "        data = file.readlines()\n",
    "    \n",
    "    # PROBLEM SPECIFICATIONS: Line #1: number of machines (M), and of jobs (J)\n",
    "    specs = data[0].split()\n",
    "    J = int(specs[0])\n",
    "    M = int(specs[1])\n",
    "\n",
    "    machine_names = data[1].split()[0::2]\n",
    "\n",
    "    prec_lines = len(data) - (J+1)*2 - 2 # Number of lines for precedences in the instance file\n",
    "\n",
    "    # PROCESSING TIMES: lines # 2 ... 2+J-1\n",
    "    skiprows_times = 1\n",
    "    skipfooter_times = prec_lines + (J + 3)\n",
    "    processing_times_df = pd.read_csv(file_name,\n",
    "                                      sep=\"\\s+\",\n",
    "                                      header=None,\n",
    "                                      names=machine_names,\n",
    "                                      index_col=False,\n",
    "                                      usecols=list(range(1,2*M,2)),\n",
    "                                      dtype=np.int8,\n",
    "                                      engine='python',\n",
    "                                      skiprows=skiprows_times,\n",
    "                                      skipfooter=skipfooter_times\n",
    "                                     )\n",
    "\n",
    "    # RESOURCES: lines # 2 + J ... 2 * (J + 1)\n",
    "    skiprows_res = J + 2\n",
    "    skipfooter_res = prec_lines + 2\n",
    "    resources_df = pd.read_csv(file_name,\n",
    "                               sep=\"\\s+\",\n",
    "                               header=None,\n",
    "                               names=machine_names,\n",
    "                               index_col=False,\n",
    "                               usecols=list(range(1,2*M,2)),\n",
    "                               dtype=np.int8,\n",
    "                               engine='python',\n",
    "                               skiprows=skiprows_res,\n",
    "                               skipfooter=skipfooter_res\n",
    "                              )\n",
    "    \n",
    "    Rmax = int(data[(J+1)*2 + 1])\n",
    "    \n",
    "    # PRECEDENCES: lines # 2 + J ... 2 * (J + 1)\n",
    "    if prec_lines > 1: # If 1 then 'Precedence' would be an empty field\n",
    "        prec_array = np.full(shape=(J,), fill_value=None)\n",
    "        for p in range(len(data)-prec_lines+1,len(data)):\n",
    "            prec_line = data[p]\n",
    "            prec_line = prec_line.split(':')\n",
    "            prec_array[int(prec_line[0])] = int(prec_line[1])\n",
    "        \n",
    "    problem_df = pd.concat([processing_times_df, resources_df, pd.Series(prec_array)], axis=1, join='outer', copy=False)    \n",
    "    \n",
    "    print(f'\\nInstance specifications: J = {J} jobs, M = {M} machines, prec = {prec_lines-1} precedence relationship(s), and Rmax = {Rmax} resources.')\n",
    "\n",
    "    return problem_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA STRUCTURING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Job:\n",
    "    def __init__(self, index, p_times, res, prec):\n",
    "        self.index = int(index) # Index (name) of the job as an int\n",
    "        self.p_times = p_times # Processing times on each machine as a NumPy array of float\n",
    "        self.res = res # Job resources on each machine as a NumPy array of float\n",
    "        self.prec = prec # Previous job (with a precedence relation) as an int\n",
    "        self.cost = int(0) # Job's assigned cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Machine:\n",
    "    def __init__(self, index):\n",
    "        self.index = int(index) # Index (name) of the machine as an int\n",
    "        self.job_seq = np.empty(shape=(0)) #, dtype=np.int8) # Job sequence on the machine as a NumPy array\n",
    "        self.job_times = np.empty(shape=(0), dtype=np.int8) # Job processing time on the machine as a NumPy array\n",
    "        self.s_times = np.empty(shape=(0), dtype=np.int8) # Job processing time on the machine as a NumPy array\n",
    "        self.e_times = np.empty(shape=(0), dtype=np.int8) # Job processing time on the machine as a NumPy array\n",
    "        self.t_spans = {}\n",
    "        self.job_res = np.empty(shape=(0), dtype=np.int8) # Job resources on the machine at every instant as a NumPy array\n",
    "        self.C = int(0) # Machine makespan as an int\n",
    "        \n",
    "    def reset(self):\n",
    "        self.job_seq = np.empty(shape=(0)) #, dtype=np.int8) # Job sequence on the machine as a NumPy array\n",
    "        self.job_times = np.empty(shape=(0), dtype=np.int8) # Job processing time on the machine as a NumPy array\n",
    "        self.s_times = np.empty(shape=(0), dtype=np.int8) # Job processing time on the machine as a NumPy array\n",
    "        self.e_times = np.empty(shape=(0), dtype=np.int8) # Job processing time on the machine as a NumPy array\n",
    "        self.t_spans.clear()\n",
    "        self.job_res = np.empty(shape=(0), dtype=np.int8) # Job resources on the machine as a NumPy array\n",
    "        self.C = int(0) # Machine makespan as an int\n",
    "        \n",
    "    def program_job(self, job, pos=-1):\n",
    "        # Every parameter's length will be:\n",
    "            # s_times, e_times, job_times, job_seq: J\n",
    "            # job_res: C-1\n",
    "        if pos == -1:\n",
    "            job_s_time = int(0) if (self.job_seq.size == 0) else int(self.C)\n",
    "            job_e_time = job_s_time + job.p_times[self.index]\n",
    "            self.s_times = np.append(self.s_times, job_s_time)\n",
    "            self.e_times = np.append(self.e_times, job_e_time)\n",
    "            self.job_times = np.append(self.job_times, job.p_times[self.index])\n",
    "            self.job_res = np.append(self.job_res, job.res[self.index]*np.ones(shape=(job.p_times[self.index],)))\n",
    "            self.job_seq = np.append(self.job_seq, job)\n",
    "        else:\n",
    "            job_s_time = int(0) if (self.job_seq.size == 0) else int(self.s_times[pos])\n",
    "            job_e_time = job_s_time + job.p_times[self.index]\n",
    "            self.s_times = np.concatenate((self.s_times[:pos], \n",
    "                                           np.array([job_s_time]), \n",
    "                                           self.s_times[pos:] + job.p_times[self.index]))\n",
    "            self.e_times = np.concatenate((self.e_times[:pos], \n",
    "                                           np.array([job_e_time]), \n",
    "                                           self.e_times[pos:] + job.p_times[self.index]))\n",
    "            self.job_times = np.concatenate((self.job_times[:pos], \n",
    "                                             np.array([job.p_times[self.index]]), \n",
    "                                             self.job_times[pos:]))\n",
    "            self.job_res = np.concatenate((self.job_res[:job_s_time], \n",
    "                                             job.res[self.index]*np.ones(shape=(job.p_times[self.index],)), \n",
    "                                             self.job_res[job_s_time:]))\n",
    "            self.job_seq = np.concatenate((self.job_seq[:pos], \n",
    "                                           np.array([job]), \n",
    "                                           self.job_seq[pos:]))\n",
    "        self.t_spans.clear()\n",
    "        for j in range(len(self.job_seq)):\n",
    "            self.t_spans.update({self.job_seq[j].index: (self.s_times[j], self.e_times[j])})\n",
    "        self.C = self.e_times[-1] # Machine makespan as an int\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_jobs(problem_df):\n",
    "    \n",
    "    # Creation of the job list\n",
    "    \n",
    "    job_list = []\n",
    "\n",
    "    for j in range(J):\n",
    "        index = j\n",
    "        p_times = np.array(list(problem_df.iloc[j,:M]))\n",
    "        res = np.array(list(problem_df.iloc[j,M:2*M]))\n",
    "        prec = int(problem_df.iloc[j,2*M]) if isinstance(problem_df.iloc[j,2*M], int) else problem_df.iloc[j,2*M]\n",
    "        job = Job(index, p_times, res, prec)\n",
    "        job_list.append(job)               \n",
    "\n",
    "    return job_list    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_machines(num_machines):\n",
    "    \n",
    "    # Creation of the machine list\n",
    "    \n",
    "    machine_list = [Machine(m) for m in range(num_machines)]\n",
    "\n",
    "    return machine_list\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA VISUALISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_solution(solution):\n",
    "    \n",
    "    # Display of a solution\n",
    "    \n",
    "    for machine in solution:\n",
    "        print(f'\\nMachine M{machine.index}:')\n",
    "        print(f'Job sequence: \\t\\t {[job.index for job in machine.job_seq]}')\n",
    "        print(f'Start times: \\t\\t {machine.s_times}')\n",
    "        print(f'Processing times: \\t {machine.job_times}')\n",
    "        print(f'End times: \\t\\t {machine.e_times}')\n",
    "        print(f'Resources: \\t\\t {machine.job_res}')\n",
    "        print(f'Makespan: \\t\\t C = {machine.C}')\n",
    "\n",
    "    C_max = max([machine.C for machine in solution])\n",
    "    print(f'\\nThe maximum makespan is C_max: {C_max}')\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLUTION FEASIBILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_feasibility(solution):\n",
    "        \n",
    "# def assess_feasibility(solution):\n",
    "    # Assesses whether a solution to the problem is feasible or unfeasible. Along with this condition, the function returns:\n",
    "        # If feasible, sol_cost = sol_Cmax.\n",
    "        # If unfeasible, sol_cost = sol_Cmax + (e_time_prec - s_time_dep) + abs(np.sum(acc_res - Rmax_res)).\n",
    "\n",
    "    # time_span_dict_m = {}\n",
    "    job_finder_dict = {}\n",
    "    dependency_dict = {}    \n",
    "\n",
    "    sol_Cmax = max([machine.C for machine in solution])\n",
    "    res_matrix = np.zeros(shape=(M,sol_Cmax))\n",
    "    sol_cost = sol_Cmax\n",
    "\n",
    "    prec_ok = True\n",
    "        \n",
    "    _ = [job_finder_dict.update({job.index: machine.index}) for machine in solution for job in machine.job_seq]\n",
    "\n",
    "    for machine in solution:\n",
    "        \n",
    "    # We iterate over every machine and create:\n",
    "        # For every job: an entry into the time_span_dict: key=job index, value=(start time, end time)\n",
    "        # For every job with a precedence relationship: an entry into the dependency_dict: key=dependent job's index, value=precedent job's index                \n",
    "        # For every job: a row in the res_matrix with the resource distribution over time (the whole time span of every machine)\n",
    "            \n",
    "        for pos in range(len(machine.job_seq)):\n",
    "            \n",
    "            if isinstance(machine.job_seq[pos].prec, int):\n",
    "                dep_job = machine.job_seq[pos].index\n",
    "                dep_machine_idx = machine.index\n",
    "                prec_job = machine.job_seq[pos].prec\n",
    "                prec_machine = job_finder_dict.get(prec_job)\n",
    "                dependency_dict.update({(dep_machine_idx,dep_job) : (prec_machine,prec_job)})\n",
    "\n",
    "        res_matrix[machine.index, :machine.C] = np.transpose(machine.job_res)\n",
    "\n",
    "    # Precedence relationships are analysed:\n",
    "        # Jobs with precedence relationships have to start once the precedent jobs have finalised.\n",
    "        # Otherwise, the prec_ok condition is set to False.\n",
    "        \n",
    "    for dep_job_info,prec_job_info in dependency_dict.items():\n",
    "        dep_machine = solution[dep_job_info[0]]\n",
    "        prec_machine = solution[prec_job_info[0]]        \n",
    "        s_time_dep = dep_machine.t_spans.get(dep_job_info[1])[0]\n",
    "        e_time_prec = prec_machine.t_spans.get(prec_job_info[1])[1]\n",
    "        if s_time_dep < e_time_prec:\n",
    "            prec_ok = False\n",
    "            sol_cost += (e_time_prec - s_time_dep)\n",
    "            break\n",
    "            \n",
    "    # Resource requirements are analysed:\n",
    "        # Cumulative use of resources in all machines at every instant of time is calculated.\n",
    "        # The res_ok condition is set to False as soon as Rmax is exceeded.\n",
    "        \n",
    "    acc_res = np.sum(res_matrix, axis=0)\n",
    "    Rmax_res = Rmax*np.ones_like(acc_res)\n",
    "    res_ok = True if np.all(acc_res <= Rmax) else False\n",
    "    sol_cost += 0 if res_ok else abs(np.sum(acc_res - Rmax_res))\n",
    "    \n",
    "    return int(sol_cost), prec_ok, res_ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpwtaRQIFYvO"
   },
   "source": [
    "# CONSTRUCTIVE PHASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_rcl(pre_sol, job_dict, rule, alpha):\n",
    "    \n",
    "    # Construction of a restricted candidate list (RCL) Pending jobs are sorted on every machine according to their cost value.\n",
    "\n",
    "    rcl_dict = {}\n",
    "    job_finder_dict = {}\n",
    "            \n",
    "    _ = [job_finder_dict.update({job.index: machine.index}) for machine in pre_sol for job in machine.job_seq]\n",
    "\n",
    "    \n",
    "    # Pending jobs are sorted on every machine according to their cost value.\n",
    "    \n",
    "    if rule == 'SPT':\n",
    "        for m in range(M):\n",
    "            for job in job_dict[m]:\n",
    "                job.cost = job.p_times[m] + pre_sol[m].C\n",
    "            rcl_dict[m] = sorted(job_dict[m], key=lambda x: x.p_times[m])\n",
    "         \n",
    "    elif rule == 'PREC':\n",
    "        for m in range(M):\n",
    "            p_time_avg = np.mean([job.p_times[m] for job in job_dict[m]])\n",
    "            for job in job_dict[m]:\n",
    "                job.cost = (job.p_times[m] + pre_sol[m].C) if not(isinstance(job.prec,int)) else (job.p_times[m] + pre_sol[m].C + p_time_avg)\n",
    "            rcl_dict[m] = sorted(job_dict[m], key=lambda x: x.cost)\n",
    "            \n",
    "    elif rule == 'RES':\n",
    "        for m in range(M):\n",
    "            p_time_avg = np.mean([job.p_times[m] for job in job_dict[m]])\n",
    "            for job in job_dict[m]:\n",
    "                job.cost = (job.p_times[m] + pre_sol[m].C + job.res[m]**2) if not(isinstance(job.prec,int)) else (job.p_times[m] + pre_sol[m].C + p_time_avg + job.res[m]**2)\n",
    "            rcl_dict[m] = sorted(job_dict[m], key=lambda x: x.cost)\n",
    "\n",
    "    \n",
    "    cost_list = [[job.cost for job in job_list_k] for k,job_list_k in job_dict.items()] # List of lists containing the pending jobs' costs on every machine\n",
    "    cost_array = np.asarray(cost_list)\n",
    "    \n",
    "    # Range of cost values to construct the RCL as a function of alpha.\n",
    "\n",
    "    cost_min = min(np.min(cost_array, axis=1))\n",
    "    cost_max = cost_min + alpha * (max(np.max(cost_array, axis=1)) - cost_min)\n",
    "    \n",
    "    # Pending jobs outside the limits of the RCL are removed.\n",
    "\n",
    "    for k,job in rcl_dict.items():\n",
    "        kj = len(rcl_dict[k]) - 1\n",
    "        while kj >= 0 and (job[kj].cost > cost_max):\n",
    "            rcl_dict[k].pop()\n",
    "            kj -= 1\n",
    "\n",
    "    return rcl_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tzrkaBS3gmmz"
   },
   "outputs": [],
   "source": [
    "def select_candidate(rcl_dict):\n",
    "    \n",
    "    candidate_machine_index = None\n",
    "    candidate_job_index = None\n",
    "    \n",
    "    candidate_machine_list = [machine_index for machine_index in rcl_dict.keys() if len(rcl_dict[machine_index]) > 0]\n",
    "    candidate_machine_index = random.choice(candidate_machine_list)\n",
    "    candidate_job_index = random.choice(rcl_dict[candidate_machine_index]).index\n",
    "\n",
    "    return candidate_machine_index, candidate_job_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "tzrkaBS3gmmz"
   },
   "outputs": [],
   "source": [
    "def construct_initial_solution(job_list, rule, alpha):\n",
    "\n",
    "    # Creation of a dictionary with items defined by:\n",
    "        # keys: indices of the machines\n",
    "        # values: list of jobs sorted by the SPT rule (processing times in non-decreasing order on each machine)    \n",
    "    \n",
    "    job_dict = {}\n",
    "    for m in range(M):\n",
    "        job_dict.update({m: copy.deepcopy(job_list)})\n",
    "    \n",
    "    pending_jobs = J\n",
    "    \n",
    "    machine_env = create_machines(M)\n",
    "    _ = [machine.reset() for machine in machine_env]    \n",
    "    \n",
    "    while pending_jobs > 0:\n",
    "        \n",
    "        rcl_dict = construct_rcl(machine_env, job_dict, rule, alpha)        \n",
    "        candidate_machine_index, candidate_job_index = select_candidate(rcl_dict)\n",
    "        \n",
    "        # Assign the SPT job to the corresponding machine and remove it from the rest\n",
    "        for machine_index,jobs in job_dict.items():            \n",
    "            job_pos = [job.index for job in jobs].index(candidate_job_index) #[0][0]\n",
    "            candidate_job = jobs.pop(job_pos)\n",
    "            if machine_index == candidate_machine_index:\n",
    "                machine_env[machine_index].program_job(candidate_job)\n",
    "       \n",
    "        pending_jobs -= 1\n",
    "        \n",
    "    initial_Cmax, initial_prec_ok, initial_res_ok  = assess_feasibility(machine_env)\n",
    "    initial_feasibility = initial_prec_ok and initial_res_ok\n",
    "\n",
    "    return machine_env, initial_Cmax, initial_prec_ok, initial_res_ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gxyEjqm8YZIk"
   },
   "source": [
    "# LOCAL SEARCH PHASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def external_insertion(initial_solution, initial_Cmax, initial_feasibility, insert_time_limit):\n",
    "    \n",
    "    # Investigate neighbourhood of the initial solution (i.e. machine_environment generated in the constructive phase)    \n",
    "    \n",
    "    insert_time = 0\n",
    "    insert_start_time = time.monotonic()\n",
    "    \n",
    "    insert_time_exceeded = False\n",
    "    \n",
    "    sol_counter = 0\n",
    "    \n",
    "    if initial_feasibility:\n",
    "        best_f_solution = copy.deepcopy(initial_solution)\n",
    "        best_f_Cmax = initial_Cmax\n",
    "        partial_prec_ok = True\n",
    "        partial_res_ok = True\n",
    "        best_u_solution = []\n",
    "        best_u_Cmax = np.inf\n",
    "    else:\n",
    "        best_f_solution = []\n",
    "        best_f_Cmax = np.inf\n",
    "        best_u_solution = copy.deepcopy(initial_solution)\n",
    "        best_u_Cmax = initial_Cmax\n",
    "        partial_prec_ok = False\n",
    "        partial_res_ok = False\n",
    "    \n",
    "    for m0 in range(M):\n",
    "        \n",
    "        for j0 in range(len(initial_solution[m0].job_seq)):\n",
    "\n",
    "            extracted_job_0 = initial_solution[m0].job_seq[j0]\n",
    "                \n",
    "            for m1 in (set(range(M))-set([m0])):\n",
    "\n",
    "                for j1 in range(len(initial_solution[m1].job_seq)+1):\n",
    "\n",
    "                    partial_solution =  copy.deepcopy(initial_solution)\n",
    "\n",
    "                    partial_solution_0 = np.concatenate((initial_solution[m0].job_seq[:j0], initial_solution[m0].job_seq[j0+1:]))\n",
    "                    partial_solution[m0].reset()\n",
    "                    for job in partial_solution_0:\n",
    "                        partial_solution[m0].program_job(job)\n",
    "\n",
    "                    partial_solution_1 = np.concatenate((initial_solution[m1].job_seq[:j1], [extracted_job_0], initial_solution[m1].job_seq[j1:]))\n",
    "                    partial_solution[m1].reset()\n",
    "                    for job in partial_solution_1:\n",
    "                        partial_solution[m1].program_job(job)\n",
    "\n",
    "                    sol_counter += 1\n",
    "                    partial_Cmax, partial_prec_ok, partial_res_ok = assess_feasibility(partial_solution)\n",
    "                    partial_feasibility = partial_prec_ok and partial_res_ok\n",
    "    \n",
    "                    if partial_feasibility:\n",
    "                        if partial_Cmax < best_f_Cmax:\n",
    "                            best_f_solution = copy.deepcopy(partial_solution)\n",
    "                            best_f_Cmax = partial_Cmax\n",
    "                    else:\n",
    "                        if partial_Cmax < best_u_Cmax:\n",
    "                            best_u_solution = copy.deepcopy(partial_solution)\n",
    "                            best_u_Cmax = partial_Cmax\n",
    "                            \n",
    "                    del partial_solution\n",
    "                    \n",
    "                    insert_end_time = time.monotonic()\n",
    "                    insert_time = insert_end_time - insert_start_time\n",
    "                    insert_time_exceeded = insert_time >= insert_time_limit\n",
    "                    \n",
    "                    if insert_time_exceeded:\n",
    "                        break\n",
    "                        \n",
    "                if insert_time_exceeded:\n",
    "                    break\n",
    "                    \n",
    "            if insert_time_exceeded:\n",
    "                break\n",
    "                \n",
    "        if insert_time_exceeded:\n",
    "            break\n",
    "                \n",
    "    return partial_prec_ok, partial_res_ok, best_f_solution, best_f_Cmax, best_u_solution, best_u_Cmax, sol_counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def external_swap(initial_solution, initial_Cmax, initial_feasibility, swap_time_limit):\n",
    "    \n",
    "    # Investigate neighbourhood of the initial solution (i.e. machine_environment generated in the constructive phase)\n",
    "    # The looping structure of the internal insertion is modified to avoid assessing twice the same swaps (e.g., 0<->1, 1<->0)\n",
    "    \n",
    "    # Timers\n",
    "    swap_time = 0\n",
    "    swap_start_time = time.monotonic()    \n",
    "    swap_time_exceeded = False\n",
    "    \n",
    "    # Solution counters\n",
    "    sol_counter = 0\n",
    "    \n",
    "    if initial_feasibility:\n",
    "        best_f_solution = copy.deepcopy(initial_solution)\n",
    "        best_f_Cmax = initial_Cmax\n",
    "        partial_prec_ok = True\n",
    "        partial_res_ok = True\n",
    "        best_u_solution = []\n",
    "        best_u_Cmax = np.inf\n",
    "    else:\n",
    "        best_f_solution = []\n",
    "        best_f_Cmax = np.inf\n",
    "        best_u_solution = copy.deepcopy(initial_solution)\n",
    "        best_u_Cmax = initial_Cmax\n",
    "        partial_prec_ok = False\n",
    "        partial_res_ok = False\n",
    "    \n",
    "    for m in range(M-1):\n",
    "        \n",
    "        m0 = m\n",
    "        \n",
    "        for j0 in range(len(initial_solution[m0].job_seq)):\n",
    "            \n",
    "            m1 = m0 + 1\n",
    "            \n",
    "            while m1 < M:\n",
    "                        \n",
    "                for j1 in range(len(initial_solution[m1].job_seq)):\n",
    "\n",
    "                    ################################################# EXTERNAL SWAP #################################################\n",
    "\n",
    "                    partial_solution =  copy.deepcopy(initial_solution)\n",
    "                    \n",
    "                    extracted_job_0 = initial_solution[m0].job_seq[j0]\n",
    "                    extracted_job_1 = initial_solution[m1].job_seq[j1]\n",
    "                    \n",
    "                    partial_solution_0 = np.concatenate((initial_solution[m0].job_seq[:j0], [extracted_job_1], initial_solution[m0].job_seq[j0+1:]))\n",
    "                    partial_solution[m0].reset()\n",
    "                    for job in partial_solution_0:\n",
    "                        partial_solution[m0].program_job(job)\n",
    "\n",
    "                    partial_solution_1 = np.concatenate((initial_solution[m1].job_seq[:j1], [extracted_job_0], initial_solution[m1].job_seq[j1+1:]))\n",
    "                    partial_solution[m1].reset()\n",
    "                    for job in partial_solution_1:\n",
    "                        partial_solution[m1].program_job(job)\n",
    "                    \n",
    "                    sol_counter += 1\n",
    "                    partial_Cmax, partial_prec_ok, partial_res_ok = assess_feasibility(partial_solution)\n",
    "                    partial_feasibility = partial_prec_ok and partial_res_ok\n",
    "\n",
    "                    if partial_feasibility:\n",
    "                        if partial_Cmax < best_f_Cmax:\n",
    "                            best_f_solution = copy.deepcopy(partial_solution)\n",
    "                            best_f_Cmax = partial_Cmax\n",
    "                    else:\n",
    "                        if partial_Cmax < best_u_Cmax:\n",
    "                            best_u_solution = copy.deepcopy(partial_solution)\n",
    "                            best_u_Cmax = partial_Cmax\n",
    "                            \n",
    "                    del partial_solution\n",
    "                    \n",
    "                    swap_end_time = time.monotonic()\n",
    "                    swap_time = swap_end_time - swap_start_time\n",
    "                    swap_time_exceeded = swap_time >= swap_time_limit\n",
    "                    \n",
    "                    if swap_time_exceeded:\n",
    "                        break\n",
    "                        \n",
    "                m1 += 1\n",
    "                \n",
    "                if swap_time_exceeded:\n",
    "                    break\n",
    "\n",
    "            if swap_time_exceeded:\n",
    "                break\n",
    "\n",
    "        if swap_time_exceeded:\n",
    "            # print('swap break level 1')\n",
    "            break\n",
    "                \n",
    "    return partial_prec_ok, partial_res_ok, best_f_solution, best_f_Cmax, best_u_solution, best_u_Cmax, sol_counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_search(initial_solution, initial_Cmax, initial_feasibility, local_time, local_time_limit, search_iter_limit, search_stagnation_limit):\n",
    "    \n",
    "    # Local search algorithm\n",
    "    \n",
    "    # Timers\n",
    "    local_time = 0    \n",
    "    local_start_time = time.monotonic()\n",
    "    \n",
    "    local_iter = 0\n",
    "    search_improvement = False\n",
    "    search_stagnation = 0\n",
    "    \n",
    "    best_feasible_dict = {}\n",
    "    best_unfeasible_dict = {}\n",
    "    \n",
    "    # Solution counters\n",
    "    local_sol_counter = 0\n",
    "    insert_counter = 0\n",
    "    swap_counter = 0\n",
    "\n",
    "    if initial_feasibility:\n",
    "        best_f_Cmax = initial_Cmax\n",
    "        best_feasible_dict.update({initial_Cmax: copy.deepcopy(initial_solution)})\n",
    "        best_u_Cmax = np.inf\n",
    "    else:\n",
    "        best_f_Cmax = np.inf\n",
    "        best_u_Cmax = initial_Cmax\n",
    "        best_unfeasible_dict.update({initial_Cmax: copy.deepcopy(initial_solution)})\n",
    "    \n",
    "    while (local_time < local_time_limit) and (search_stagnation < search_stagnation_limit):\n",
    "        \n",
    "        # External insertion\n",
    "        insert_time_limit = (local_time_limit - local_time) / 2\n",
    "        partial_prec_ok, partial_res_ok, partial_f_sol, partial_f_Cmax, partial_u_sol, partial_u_Cmax, insert_counter = external_insertion(initial_solution, \n",
    "                                                                                                                        initial_Cmax, \n",
    "                                                                                                                        initial_feasibility, \n",
    "                                                                                                                        insert_time_limit)\n",
    "        local_sol_counter += insert_counter\n",
    "\n",
    "        # Update solutions, if necessary. With the following structure, we prioritise feasible solutions (with respect to unfeasible ones)\n",
    "        if (len(partial_u_sol) > 0) and (partial_u_Cmax < best_u_Cmax):\n",
    "            best_unfeasible_dict.update({partial_u_Cmax: partial_u_sol})\n",
    "            best_u_Cmax = partial_u_Cmax\n",
    "            initial_solution = partial_u_sol\n",
    "            initial_Cmax = partial_u_Cmax\n",
    "            initial_feasibility = False\n",
    "            search_improvement = True\n",
    "        if (len(partial_f_sol) > 0) and (partial_f_Cmax < best_f_Cmax):\n",
    "            best_feasible_dict.update({partial_f_Cmax: partial_f_sol})\n",
    "            best_f_Cmax = partial_f_Cmax\n",
    "            initial_solution = partial_f_sol\n",
    "            initial_Cmax = partial_f_Cmax\n",
    "            initial_feasibility = True\n",
    "            search_improvement = True\n",
    "\n",
    "        # External swap\n",
    "        swap_time_limit = (local_time_limit - local_time) / 2\n",
    "        partial_prec_ok, partial_res_ok, partial_f_sol, partial_f_Cmax, partial_u_sol, partial_u_Cmax, swap_counter = external_swap(initial_solution, \n",
    "                                                                                                                                    initial_Cmax, \n",
    "                                                                                                                                    initial_feasibility, \n",
    "                                                                                                                                    swap_time_limit)\n",
    "        local_sol_counter += swap_counter\n",
    "\n",
    "        # Update solutions, if necessary. With the following structure, we prioritise feasible solutions (with respect to unfeasible ones)\n",
    "        if (len(partial_u_sol) > 0) and (partial_u_Cmax < best_u_Cmax):\n",
    "            best_unfeasible_dict.update({partial_u_Cmax: partial_u_sol})\n",
    "            best_u_Cmax = partial_u_Cmax\n",
    "            initial_solution = partial_u_sol\n",
    "            initial_Cmax = partial_u_Cmax\n",
    "            initial_feasibility = False\n",
    "            search_improvement = True\n",
    "        if (len(partial_f_sol) > 0) and (partial_f_Cmax < best_f_Cmax):\n",
    "            best_feasible_dict.update({partial_f_Cmax: partial_f_sol})\n",
    "            best_f_Cmax = partial_f_Cmax\n",
    "            initial_solution = partial_f_sol\n",
    "            initial_Cmax = partial_f_Cmax\n",
    "            initial_feasibility = True\n",
    "            search_improvement = True\n",
    "            \n",
    "        if not search_improvement:\n",
    "            search_stagnation += 1\n",
    "        else:\n",
    "            search_stagnation = 0\n",
    "            search_improvement = False\n",
    "        \n",
    "        local_iter += 1\n",
    "\n",
    "        local_end_time = time.monotonic()\n",
    "        local_time += (local_end_time - local_start_time)\n",
    "        \n",
    "    best_prec_ok = partial_prec_ok\n",
    "    best_res_ok = partial_res_ok\n",
    "        \n",
    "    return best_prec_ok, best_res_ok, best_feasible_dict, best_unfeasible_dict, local_iter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLUTION REPAIRING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def internal_swap(unsorted_sol, m_index, dep_job_index, prec_job_index):\n",
    "    \n",
    "    # This function is called only when a dependent job is processed before its precedent job \n",
    "    sorted_sol = copy.deepcopy(unsorted_sol)\n",
    "    sorted_sol[m_index].reset()\n",
    "    \n",
    "    unsorted_job_list = [job for job in unsorted_sol[m_index].job_seq]\n",
    "    unsorted_job_list_indices = [job.index for job in unsorted_job_list]\n",
    "\n",
    "    dep_job_pos = unsorted_job_list_indices.index(dep_job_index)\n",
    "    prec_job_pos = unsorted_job_list_indices.index(prec_job_index)\n",
    "\n",
    "    # Firstly, jobs before the dependent one are re-programmed as they were\n",
    "    for pos, idx in enumerate(unsorted_job_list_indices):\n",
    "        \n",
    "        if idx == dep_job_index:\n",
    "            p = prec_job_pos\n",
    "        elif idx == prec_job_index:\n",
    "            p = dep_job_pos\n",
    "        else:\n",
    "            p = pos\n",
    "        \n",
    "        j = unsorted_job_list[p]\n",
    "            \n",
    "        sorted_sol[m_index].program_job(j)\n",
    "    \n",
    "    return sorted_sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repair(unfeasible_dict, repaired_prec_ok, repaired_res_ok, repair_time_limit):\n",
    "        \n",
    "    # To repair an unfeasible solution:\n",
    "        # Soluctions with precedence relationships are sought\n",
    "        # Starting times times of those jobs are compared against the finishing times of the precedent jobs \n",
    "    \n",
    "    repair_iter = 0\n",
    "    repair_time = 0\n",
    "    repair_prep_start_time = time.monotonic()\n",
    "    \n",
    "    job_finder_dict = {}\n",
    "    time_span_dict_m = {}\n",
    "    dependency_dict = {}\n",
    "    dependency_list = []\n",
    "    repaired_dict = {}\n",
    "    \n",
    "    best_u_Cmax = min(unfeasible_dict.keys())\n",
    "    unfeasible_sol = unfeasible_dict[best_u_Cmax]\n",
    "    repaired_sol = copy.deepcopy(unfeasible_sol)\n",
    "\n",
    "    sol_Cmax = max([machine.C for machine in unfeasible_sol])  \n",
    "    res_matrix = np.zeros(shape=(M,sol_Cmax))\n",
    "\n",
    "    # Precedence relationships are stored in a dictionary (dependency_dict) with the following info:\n",
    "        # key: a tuple comprising the dependent job's machine index and job index\n",
    "        # value: a tuple comprising a boolean stating whether both jobs are in the same machine, and the precedent job's index\n",
    "    # Parallelly, jobs' start and end times are stored as tuples in another dictionary (time_span_dict_m)\n",
    "    # Finally, the resources required by every job at every instant of time are stored in a matrix (res_matrix)\n",
    "        \n",
    "    _ = [job_finder_dict.update({job.index: machine.index}) for machine in unfeasible_sol for job in machine.job_seq]\n",
    "\n",
    "    for machine in unfeasible_sol:\n",
    "\n",
    "        for pos in range(len(machine.job_seq)):\n",
    "\n",
    "            if isinstance(machine.job_seq[pos].prec, int):\n",
    "                dep_job = machine.job_seq[pos].index\n",
    "                dep_machine_idx = machine.index\n",
    "\n",
    "                prec_job = machine.job_seq[pos].prec\n",
    "                prec_machine = job_finder_dict.get(prec_job)\n",
    "\n",
    "                dependency_dict[(dep_machine_idx,dep_job)] = (prec_machine,prec_job)\n",
    "\n",
    "                # A list of lists (dependency_list) is created to store interdependencies:\n",
    "                    # In the simplest case, every item (list) will contain two elements: dep_job, prec_job\n",
    "                    # In more complex situations, every item (list) will contain multiple elements: dep_job, prec_job_1, prec_job_2...\n",
    "\n",
    "                dependency_link = False\n",
    "\n",
    "                for s in range(len(dependency_list)):\n",
    "                    if dep_job in dependency_list[s]:\n",
    "                        dependency_list[s].append(prec_job)\n",
    "                        dependency_link = True\n",
    "                    elif prec_job in dependency_list[s]:\n",
    "                        # print(f'prec_job {prec_job} in dependency_list[{s}]')\n",
    "                        dependency_list[s].insert(0, dep_job)\n",
    "                        dependency_link = True\n",
    "\n",
    "                if not dependency_link:\n",
    "                    dependency_list.append([dep_job, prec_job])\n",
    "                    dependency_link = False                    \n",
    "\n",
    "        res_matrix[machine.index, :machine.C] = np.transpose(machine.job_res)\n",
    "\n",
    "    # A reversed version of the dependency_list is created:\n",
    "        # The first element [0] in every item (list) is the firt precedent job which will not be shifted\n",
    "        # The following elements [1:] in every item (list) will be ncrementally shifted\n",
    "\n",
    "    reversed_dependency_list = copy.deepcopy(dependency_list)\n",
    "\n",
    "    for item in reversed_dependency_list:\n",
    "        item.reverse()\n",
    "\n",
    "    # Precedence relationships are revised:\n",
    "        # If both jobs are in the same machine, and the dependent job is processed before the precedent job, they are immediately swapped\n",
    "        # In other cases, the precedence relationships will be fixed afterwards, whenever necessary\n",
    "\n",
    "    # The following processing would not be necessary if only simple precedence relationships existed. \n",
    "    # However, it becomes necessary when complex precedence relationships may arise.\n",
    "\n",
    "    dep_job_info_list = list(dependency_dict.keys()) # list of tuples (machine, dep_job)\n",
    "\n",
    "    idle_job = Job(index=int(J), p_times=np.ones(shape=(M,), dtype=np.int8), res=np.zeros(shape=(M,), dtype=np.int8), prec=None)\n",
    "    \n",
    "    repair_prep_end_time = time.monotonic()\n",
    "    repair_prep_time = (repair_prep_end_time - repair_prep_start_time)\n",
    "    repair_time += repair_prep_time\n",
    "    \n",
    "    repaired_sol_feasibility = repaired_prec_ok and repaired_res_ok\n",
    "    \n",
    "    print('')\n",
    "\n",
    "    while (repair_time < repair_time_limit) and (not repaired_sol_feasibility):\n",
    "\n",
    "        repair_iter += 1\n",
    "        \n",
    "        print(f'Repair iteration starting at {round(repair_time, 4)} / {repair_time_limit} s        ', end='\\r')\n",
    "        \n",
    "        if not repaired_prec_ok:\n",
    "\n",
    "            ############################################### PRECEDENCE RELATIONSHIPS ###############################################\n",
    "\n",
    "            repair_prec_start_time = time.monotonic()\n",
    "            \n",
    "            for dep_seq in reversed_dependency_list:\n",
    "\n",
    "                for i in dep_seq[1:]:\n",
    "\n",
    "                    pos_list = [True if i == dep_job_tuple[1] else False for dep_job_tuple in dep_job_info_list]\n",
    "                    pos = pos_list.index(True)\n",
    "\n",
    "                    dep_job_info = dep_job_info_list[pos]\n",
    "                    prec_job_info = dependency_dict.get(dep_job_info)\n",
    "                    \n",
    "                    dep_machine = repaired_sol[dep_job_info[0]]\n",
    "                    prec_machine = repaired_sol[prec_job_info[0]]\n",
    "                    \n",
    "                    s_time_dep = dep_machine.t_spans.get(dep_job_info[1])[0]\n",
    "                    e_time_dep = dep_machine.t_spans.get(dep_job_info[1])[1]\n",
    "                    s_time_prec = prec_machine.t_spans.get(prec_job_info[1])[0]\n",
    "                    e_time_prec = prec_machine.t_spans.get(prec_job_info[1])[1]\n",
    "                    \n",
    "                    if s_time_dep < e_time_prec:\n",
    "\n",
    "                        if dep_machine == prec_machine:\n",
    "                            repaired_sol = internal_swap(repaired_sol, dep_job_info[0], dep_job_info[1], prec_job_info[1])\n",
    "                        else:\n",
    "                            job_indices = [j.index for j in dep_machine.job_seq]\n",
    "                            pos_dep_job = job_indices.index(dep_job_info[1])\n",
    "                            for d in range(e_time_prec-s_time_dep):\n",
    "                                dep_machine.program_job(idle_job, int(pos_dep_job))                            \n",
    "\n",
    "                        # Update the resource matrix, and ime spans in the corresponding dictionary:\n",
    "                        sol_Cmax = max([machine.C for machine in repaired_sol])\n",
    "                        res_matrix = np.zeros(shape=(M,sol_Cmax))\n",
    "\n",
    "                        for machine in repaired_sol:\n",
    "                            res_matrix[machine.index, :machine.C] = np.transpose(machine.job_res)\n",
    "\n",
    "                _, repaired_prec_ok, repaired_res_ok = assess_feasibility(repaired_sol)\n",
    "                repaired_sol_feasibility = repaired_prec_ok and repaired_res_ok\n",
    "\n",
    "                if repaired_prec_ok:\n",
    "                    break\n",
    "            \n",
    "            repair_prec_end_time = time.monotonic()\n",
    "            repair_prec_time = (repair_prec_end_time - repair_prec_start_time)\n",
    "            repair_time += repair_prec_time\n",
    "            \n",
    "        elif not repaired_res_ok:\n",
    "\n",
    "            ####################################################### RESOURCES #######################################################\n",
    "\n",
    "            # Resources exceeded: This happens when a new job starts\n",
    "            \n",
    "            repair_res_start_time = time.monotonic()\n",
    "\n",
    "            acc_res = np.sum(res_matrix, axis=0)\n",
    "            repaired_res_ok = True if np.all(acc_res <= Rmax) else False\n",
    "\n",
    "            p_x_res = np.where(acc_res > Rmax)[0][0] # Index where maximum resources are first exceeded. It will always be ind_x_res <= Cmax.\n",
    "\n",
    "            candidate_machine_idx = [machine.index for machine in repaired_sol if (p_x_res in machine.s_times)]\n",
    "            candidate_machine_C = [machine.C if (machine.index in candidate_machine_idx) else np.inf for machine in repaired_sol]\n",
    "            candidate_machine_C_idxd = list(enumerate(candidate_machine_C))\n",
    "            sorted_candidate_machine_C_idxd = sorted(candidate_machine_C_idxd, key=lambda x: x[1])\n",
    "\n",
    "            m_idx = 0\n",
    "\n",
    "            while (acc_res[p_x_res] > Rmax) and (sorted_candidate_machine_C_idxd[m_idx][1] < np.inf):\n",
    "                idle_m_index = sorted_candidate_machine_C_idxd[m_idx][0]\n",
    "                idle_j_pos = np.where(repaired_sol[idle_m_index].s_times == p_x_res)[0][0]\n",
    "            \n",
    "                repaired_sol[idle_m_index].program_job(idle_job, int(idle_j_pos))\n",
    "                sol_Cmax = max([machine.C for machine in repaired_sol])\n",
    "                \n",
    "                res_matrix = np.zeros(shape=(M,sol_Cmax))\n",
    "            \n",
    "                for machine in repaired_sol:\n",
    "    \n",
    "                    res_matrix[machine.index, :machine.C] = np.transpose(machine.job_res)               \n",
    "                \n",
    "                acc_res = np.sum(res_matrix, axis=0)\n",
    "            \n",
    "                m_idx += 1\n",
    "            \n",
    "            _, repaired_prec_ok, repaired_res_ok = assess_feasibility(repaired_sol)\n",
    "            repaired_sol_feasibility = repaired_prec_ok and repaired_res_ok\n",
    "        \n",
    "            repair_res_end_time = time.monotonic()\n",
    "            repair_res_time = (repair_res_end_time - repair_res_start_time)\n",
    "            repair_time += repair_res_time\n",
    "\n",
    "    repaired_dict.update({sol_Cmax : repaired_sol})    \n",
    "\n",
    "    return repaired_dict, repaired_prec_ok, repaired_res_ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_solution(solution):\n",
    "    \n",
    "    # Generate a solution\n",
    "\n",
    "    solution_dict = {}\n",
    "    len_job_seq = []\n",
    "    \n",
    "    for machine in solution:\n",
    "        job_list = [job.index for job in machine.job_seq]\n",
    "        start_times = machine.s_times\n",
    "        job_tuples = [(start_times[i], job_list[i]) for i in range(len(job_list)) if job_list[i] != J]\n",
    "        solution_dict.update({machine.index: job_tuples})\n",
    "        len_job_seq.append(len(job_tuples))\n",
    "        \n",
    "    \n",
    "    for k,v in solution_dict.items():\n",
    "        append_v = np.nan*np.ones(max(len_job_seq) - len(v))\n",
    "        new_v = v + append_v.tolist()\n",
    "        solution_dict.update({k:new_v})\n",
    "        \n",
    "    solution_df = pd.DataFrame.from_dict(solution_dict, orient='columns')\n",
    "    \n",
    "    return solution_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN PROGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "RIXYkDWcfmwd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "instance_path: C:\\Users\\Usuario\\notebooks\\09MIAR\\input_data\\debug\n",
      "\n",
      "instances: ['006x2_1-10_1_6.txt', '008x2_1-10_1_6.txt', '010x2_1-10_1_6.txt', '050x15_1-10_1_6.txt']\n",
      "\n",
      "\n",
      "############################################################## 006x2_1-10_1_6 #############################################################\n",
      "\n",
      "Instance specifications: J = 6 jobs, M = 2 machines, prec = 1 precedence relationship(s), and Rmax = 3 resources.\n",
      "\n",
      "Instance_time_limit: 12.0\n",
      "Iter_time_limit: 6.0\n",
      "GRASP iteration No. 16 starting at 10.594 / 12.0 s        \n",
      "Repair iteration starting at 0.0 / 6.0 s        \n",
      "Best solution: Cmax: 52, feasibility: True (precedence compliance: True, and resource compliance True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0, 5)</td>\n",
       "      <td>(0, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(34, 3)</td>\n",
       "      <td>(12, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>(24, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>(34, 1)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0        1\n",
       "0   (0, 5)   (0, 4)\n",
       "1  (34, 3)  (12, 0)\n",
       "2      NaN  (24, 2)\n",
       "3      NaN  (34, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "############################################################## 008x2_1-10_1_6 #############################################################\n",
      "\n",
      "Instance specifications: J = 8 jobs, M = 2 machines, prec = 1 precedence relationship(s), and Rmax = 3 resources.\n",
      "\n",
      "Instance_time_limit: 16.0\n",
      "Iter_time_limit: 8.0\n",
      "GRASP iteration No. 11 starting at 15.411 / 16.0 s        \n",
      "Best solution: Cmax: 66, feasibility: True (precedence compliance: True, and resource compliance True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0, 6)</td>\n",
       "      <td>(0, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(17, 1)</td>\n",
       "      <td>(15, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(35, 2)</td>\n",
       "      <td>(29, 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(53, 7)</td>\n",
       "      <td>(47, 3)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0        1\n",
       "0   (0, 6)   (0, 4)\n",
       "1  (17, 1)  (15, 0)\n",
       "2  (35, 2)  (29, 5)\n",
       "3  (53, 7)  (47, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "############################################################## 010x2_1-10_1_6 #############################################################\n",
      "\n",
      "Instance specifications: J = 10 jobs, M = 2 machines, prec = 1 precedence relationship(s), and Rmax = 4 resources.\n",
      "\n",
      "Instance_time_limit: 20.0\n",
      "Iter_time_limit: 10.0\n",
      "GRASP iteration No. 8 starting at 19.251 / 20.0 s        \n",
      "Best solution: Cmax: 82, feasibility: True (precedence compliance: True, and resource compliance True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0, 9)</td>\n",
       "      <td>(0, 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(13, 2)</td>\n",
       "      <td>(13, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(29, 3)</td>\n",
       "      <td>(26, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(44, 8)</td>\n",
       "      <td>(37, 4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(60, 6)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(70, 1)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0        1\n",
       "0   (0, 9)   (0, 5)\n",
       "1  (13, 2)  (13, 0)\n",
       "2  (29, 3)  (26, 7)\n",
       "3  (44, 8)  (37, 4)\n",
       "4  (60, 6)      NaN\n",
       "5  (70, 1)      NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "############################################################# 050x15_1-10_1_6 #############################################################\n",
      "\n",
      "Instance specifications: J = 50 jobs, M = 15 machines, prec = 3 precedence relationship(s), and Rmax = 4 resources.\n",
      "\n",
      "Instance_time_limit: 750.0\n",
      "Iter_time_limit: 50.0\n",
      "GRASP iteration No. 7 starting at 568.688 / 750.0 s        \n",
      "Repair iteration starting at 0.25 / 50.0 s         \n",
      "Best solution: Cmax: 173, feasibility: True (precedence compliance: True, and resource compliance True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(10, 12)</td>\n",
       "      <td>(13, 44)</td>\n",
       "      <td>(20, 2)</td>\n",
       "      <td>(40, 49)</td>\n",
       "      <td>(0, 6)</td>\n",
       "      <td>(23, 14)</td>\n",
       "      <td>(0, 27)</td>\n",
       "      <td>(93, 24)</td>\n",
       "      <td>(161, 20)</td>\n",
       "      <td>(44, 43)</td>\n",
       "      <td>(0, 16)</td>\n",
       "      <td>(20, 17)</td>\n",
       "      <td>(34, 0)</td>\n",
       "      <td>(0, 41)</td>\n",
       "      <td>(30, 33)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(41, 22)</td>\n",
       "      <td>(54, 36)</td>\n",
       "      <td>(64, 46)</td>\n",
       "      <td>(98, 1)</td>\n",
       "      <td>(10, 37)</td>\n",
       "      <td>(81, 30)</td>\n",
       "      <td>(51, 42)</td>\n",
       "      <td>(137, 10)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(88, 40)</td>\n",
       "      <td>(18, 5)</td>\n",
       "      <td>(64, 15)</td>\n",
       "      <td>(76, 25)</td>\n",
       "      <td>(31, 11)</td>\n",
       "      <td>(70, 26)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(103, 35)</td>\n",
       "      <td>(149, 47)</td>\n",
       "      <td>(115, 48)</td>\n",
       "      <td>(153, 45)</td>\n",
       "      <td>(52, 29)</td>\n",
       "      <td>(140, 21)</td>\n",
       "      <td>(109, 4)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(129, 9)</td>\n",
       "      <td>(37, 31)</td>\n",
       "      <td>(109, 18)</td>\n",
       "      <td>(127, 28)</td>\n",
       "      <td>(75, 8)</td>\n",
       "      <td>(124, 39)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(163, 3)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(141, 23)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(56, 19)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(152, 32)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(76, 34)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(86, 7)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(104, 13)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(124, 38)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0          1          2          3          4          5         6   \\\n",
       "0   (10, 12)   (13, 44)    (20, 2)   (40, 49)     (0, 6)   (23, 14)   (0, 27)   \n",
       "1   (41, 22)   (54, 36)   (64, 46)    (98, 1)   (10, 37)   (81, 30)  (51, 42)   \n",
       "2  (103, 35)  (149, 47)  (115, 48)  (153, 45)   (52, 29)  (140, 21)  (109, 4)   \n",
       "3   (163, 3)        NaN        NaN        NaN  (141, 23)        NaN       NaN   \n",
       "4        NaN        NaN        NaN        NaN        NaN        NaN       NaN   \n",
       "5        NaN        NaN        NaN        NaN        NaN        NaN       NaN   \n",
       "6        NaN        NaN        NaN        NaN        NaN        NaN       NaN   \n",
       "7        NaN        NaN        NaN        NaN        NaN        NaN       NaN   \n",
       "\n",
       "          7          8         9          10         11         12         13  \\\n",
       "0   (93, 24)  (161, 20)  (44, 43)    (0, 16)   (20, 17)    (34, 0)    (0, 41)   \n",
       "1  (137, 10)        NaN  (88, 40)    (18, 5)   (64, 15)   (76, 25)   (31, 11)   \n",
       "2        NaN        NaN  (129, 9)   (37, 31)  (109, 18)  (127, 28)    (75, 8)   \n",
       "3        NaN        NaN       NaN   (56, 19)        NaN        NaN  (152, 32)   \n",
       "4        NaN        NaN       NaN   (76, 34)        NaN        NaN        NaN   \n",
       "5        NaN        NaN       NaN    (86, 7)        NaN        NaN        NaN   \n",
       "6        NaN        NaN       NaN  (104, 13)        NaN        NaN        NaN   \n",
       "7        NaN        NaN       NaN  (124, 38)        NaN        NaN        NaN   \n",
       "\n",
       "          14  \n",
       "0   (30, 33)  \n",
       "1   (70, 26)  \n",
       "2  (124, 39)  \n",
       "3        NaN  \n",
       "4        NaN  \n",
       "5        NaN  \n",
       "6        NaN  \n",
       "7        NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "################################################################# SUMMARY #################################################################\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alpha</th>\n",
       "      <th>T_limit</th>\n",
       "      <th>T_factor</th>\n",
       "      <th>Makespan</th>\n",
       "      <th>Feasibility</th>\n",
       "      <th>Repair</th>\n",
       "      <th>Time2best</th>\n",
       "      <th>Sol2best</th>\n",
       "      <th>Total_sol</th>\n",
       "      <th>GRASP iter</th>\n",
       "      <th>Construct_time</th>\n",
       "      <th>Search_time</th>\n",
       "      <th>Repair_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>006x2_1-10_1_6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1.406</td>\n",
       "      <td>128</td>\n",
       "      <td>128</td>\n",
       "      <td>16</td>\n",
       "      <td>0.016</td>\n",
       "      <td>1.390</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>008x2_1-10_1_6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2</td>\n",
       "      <td>66</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.282</td>\n",
       "      <td>10</td>\n",
       "      <td>112</td>\n",
       "      <td>11</td>\n",
       "      <td>0.031</td>\n",
       "      <td>3.001</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>010x2_1-10_1_6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2</td>\n",
       "      <td>82</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.703</td>\n",
       "      <td>14</td>\n",
       "      <td>112</td>\n",
       "      <td>8</td>\n",
       "      <td>0.016</td>\n",
       "      <td>5.516</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>050x15_1-10_1_6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>15</td>\n",
       "      <td>173</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>190.156</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>0.766</td>\n",
       "      <td>189.140</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Alpha  T_limit  T_factor  Makespan  Feasibility  Repair  \\\n",
       "006x2_1-10_1_6     0.0     12.0         2        52         True    True   \n",
       "008x2_1-10_1_6     0.0     16.0         2        66         True   False   \n",
       "010x2_1-10_1_6     0.0     20.0         2        82         True   False   \n",
       "050x15_1-10_1_6    0.0    750.0        15       173         True    True   \n",
       "\n",
       "                 Time2best  Sol2best  Total_sol  GRASP iter  Construct_time  \\\n",
       "006x2_1-10_1_6       1.406       128        128          16           0.016   \n",
       "008x2_1-10_1_6       0.282        10        112          11           0.031   \n",
       "010x2_1-10_1_6       0.703        14        112           8           0.016   \n",
       "050x15_1-10_1_6    190.156        35         35           7           0.766   \n",
       "\n",
       "                 Search_time  Repair_time  \n",
       "006x2_1-10_1_6         1.390         0.00  \n",
       "008x2_1-10_1_6         3.001         0.00  \n",
       "010x2_1-10_1_6         5.516         0.00  \n",
       "050x15_1-10_1_6      189.140         0.25  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'C:\\Users\\Usuario\\notebooks\\09MIAR\\output_data\\output_debug\\input_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 281\u001b[0m\n\u001b[0;32m    278\u001b[0m         output_name \u001b[38;5;241m=\u001b[39m output_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_T\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(T_index) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_a\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(alpha_index) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    280\u001b[0m         output_file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_path, output_name)\n\u001b[1;32m--> 281\u001b[0m         \u001b[43msummary_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m;\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m         os\u001b[38;5;241m.\u001b[39mchdir(instance_path)\n\u001b[0;32m    285\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(parent_path)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:3720\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3709\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3711\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3712\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3713\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3717\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3718\u001b[0m )\n\u001b[1;32m-> 3720\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3723\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3725\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3737\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py:1189\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1168\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1170\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1171\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1172\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1187\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1188\u001b[0m )\n\u001b[1;32m-> 1189\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1192\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:241\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    249\u001b[0m \n\u001b[0;32m    250\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    252\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    253\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    259\u001b[0m     )\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:734\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[1;32m--> 734\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    738\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:597\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    595\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'C:\\Users\\Usuario\\notebooks\\09MIAR\\output_data\\output_debug\\input_data'"
     ]
    }
   ],
   "source": [
    "\n",
    "################################################################ ALGORITHM CONFIGURATION ################################################################\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\Usuario\\\\notebooks\\\\09MIAR') # Home\n",
    "# os.chdir('C:\\\\Users\\\\jbb\\\\Notebooks') # Work\n",
    "parent_path = os.getcwd() # 'C:\\\\Users\\\\Usuario\\\\notebooks\\\\09MIAR'\n",
    "# print(f'\\nparent_path: {parent_path}')\n",
    "\n",
    "debug_path = os.path.join(parent_path, 'input_data', 'debug')\n",
    "cal_path = os.path.join(parent_path, 'input_data', 'cal')\n",
    "test_small_path = os.path.join(parent_path, 'input_data', 'test_small')\n",
    "test_large_path = os.path.join(parent_path, 'input_data', 'test_large')\n",
    "\n",
    "instance_path = debug_path\n",
    "print(f'\\ninstance_path: {instance_path}')\n",
    "\n",
    "entries = os.listdir(instance_path)\n",
    "# data_folders = [e for e in entries if (os.path.isdir(instance_path + '/' + e) and not(e.startswith('.ipynb')))]\n",
    "# print(f'\\ndata_fonlders: {data_folders}')\n",
    "instances = [i for i in entries if (os.path.isfile(instance_path + '/' + i) and i.endswith('.txt'))]\n",
    "print(f'\\ninstances: {instances}')\n",
    "\n",
    "os.chdir(instance_path)\n",
    "\n",
    "if instance_path == debug_path:\n",
    "\n",
    "    alpha_dict = {0: 0.00}\n",
    "    T_indices = list(range(1))\n",
    "    # T_indices = [2]\n",
    "    # alpha_dict = {0: 0.28}\n",
    "    # T_indices = list(range(3))\n",
    "    output_path = os.path.join(parent_path, 'output_data', 'output_debug')\n",
    "\n",
    "elif instance_path == cal_path:\n",
    "\n",
    "    alpha_dict = {0: 0.00, 1: 0.25, 2: 0.50, 3: 0.75, 4: 1.00}\n",
    "    T_indices = list(range(3))\n",
    "    # T_indices = [2]\n",
    "    # alpha_dict = {0: 0.28}\n",
    "    # T_indices = list(range(3))\n",
    "    output_path = os.path.join(parent_path, 'output_data', 'output_cal')\n",
    "\n",
    "for alpha_index, problem_alpha in alpha_dict.items():\n",
    "    \n",
    "    for T_index in T_indices:\n",
    "\n",
    "        instance_sol = {}\n",
    "\n",
    "        for instance in instances:\n",
    "\n",
    "            instance_name = instance.rstrip('.txt')\n",
    "\n",
    "            print('\\n\\n' + str(' ' + instance_name + ' ').center(139, '#')) # 483\n",
    "\n",
    "            ##################################################### DATA LOADING #####################################################\n",
    "\n",
    "            load_start_time = time.monotonic()\n",
    "\n",
    "            problem_df = read_instance(instance)\n",
    "\n",
    "            load_end_time = time.monotonic()\n",
    "\n",
    "            load_time = (load_end_time - load_start_time)\n",
    "\n",
    "            ##################################################### MULTI-START #####################################################\n",
    "\n",
    "            random.seed(42)\n",
    "            g_rule = 'RES'\n",
    "\n",
    "            if instance_path == debug_path:\n",
    "                T_dict = {0: M, 1: 2, 2: 1}\n",
    "            elif instance_path == cal_path:\n",
    "                T_dict = {0: M, 1: 2, 2: 1}\n",
    "\n",
    "            search_iter_limit = J\n",
    "            search_stagnation_limit = J # J / 2\n",
    "\n",
    "            # Solution dictionaries\n",
    "            feasible_sol_dict = {}\n",
    "            unfeasible_sol_dict = {}\n",
    "            repaired_sol_dict = {}\n",
    "            best_grasp_f_dict = {}\n",
    "            best_grasp_u_dict = {}\n",
    "            best_grasp_sol_dict = {}\n",
    "\n",
    "            best_grasp_f_Cmax = np.inf\n",
    "            best_grasp_f_sol = []\n",
    "            best_grasp_f_dict.update({best_grasp_f_Cmax: best_grasp_f_sol})\n",
    "\n",
    "            best_grasp_u_Cmax = np.inf\n",
    "            best_grasp_u_sol = []\n",
    "            best_grasp_u_dict.update({best_grasp_u_Cmax: best_grasp_u_sol})\n",
    "\n",
    "            # Solution output\n",
    "            instance_Cmax = np.inf\n",
    "            instance_sol_df = pd.DataFrame()\n",
    "\n",
    "            # Iteration counters\n",
    "            grasp_iter = 0\n",
    "            grasp_improvement = False\n",
    "            grasp_stagnation = 0\n",
    "\n",
    "            # Solution counters\n",
    "            grasp_sol_counter_cum = 0\n",
    "            construct_sol_counter_cum = 0\n",
    "            search_sol_counter_cum = 0\n",
    "            repair_sol_counter_cum = 0\n",
    "            best_sol_counter_cum = 0\n",
    "\n",
    "            # Timers\n",
    "            grasp_time = 0\n",
    "            construct_time = 0\n",
    "            search_time = 0\n",
    "            repair_time = 0\n",
    "            best_time = 0\n",
    "\n",
    "            instance_T = float(J * M)\n",
    "            T_factor = T_dict.get(T_index)\n",
    "            iter_T = float(J * M) / T_factor\n",
    "            print(f'\\nInstance_time_limit: {instance_T}\\nIter_time_limit: {iter_T}')\n",
    "\n",
    "            best_start_time = time.monotonic()\n",
    "\n",
    "            while grasp_time < instance_T:\n",
    "\n",
    "                grasp_iter_start_time = time.monotonic()\n",
    "                grasp_iter += 1\n",
    "\n",
    "                print(f'GRASP iteration No. {grasp_iter} starting at {round(grasp_time, 4)} / {instance_T} s        ', end='\\r')\n",
    "\n",
    "                ################################################ CONSTRUCTIVE PHASE ################################################\n",
    "\n",
    "                construct_start_time = time.monotonic()\n",
    "\n",
    "                job_list = create_jobs(problem_df)\n",
    "\n",
    "                # Every GRASP iteration starts with its own initial solution regardless whether it improves or not the previous ones.\n",
    "                # The purpose is to explore other areas of the solution space.\n",
    "\n",
    "                initial_solution, initial_Cmax, initial_prec_ok, initial_res_ok = construct_initial_solution(job_list, g_rule, problem_alpha)\n",
    "                initial_feasibility = initial_prec_ok and initial_res_ok\n",
    "                construct_sol_counter_cum += 1\n",
    "\n",
    "                if (initial_feasibility) and (initial_Cmax < best_grasp_f_Cmax):\n",
    "                    feasible_sol_dict.update({initial_Cmax: initial_solution})\n",
    "                    best_sol_counter_cum = construct_sol_counter_cum + search_sol_counter_cum\n",
    "                    best_end_time = time.monotonic()\n",
    "                    best_time = (best_end_time - best_start_time)\n",
    "                elif (not initial_feasibility) and (initial_Cmax < best_grasp_u_Cmax):\n",
    "                    unfeasible_sol_dict.update({initial_Cmax: initial_solution})        \n",
    "\n",
    "                construct_end_time = time.monotonic()\n",
    "                construct_time += (construct_end_time - construct_start_time)\n",
    "\n",
    "                grasp_time += construct_time\n",
    "\n",
    "                ################################################ lOCAL SEARCH PHASE ################################################\n",
    "\n",
    "                search_start_time = time.monotonic()\n",
    "\n",
    "                neighbour_prec_ok, neighbour_res_ok, neighbour_feasible_dict, neighbour_unfeasible_dict, search_sol_counter = local_search(initial_solution, \n",
    "                                                                                                                                          initial_Cmax, \n",
    "                                                                                                                                          initial_feasibility, \n",
    "                                                                                                                                          grasp_time, \n",
    "                                                                                                                                          iter_T, \n",
    "                                                                                                                                          search_iter_limit, \n",
    "                                                                                                                                          search_stagnation_limit)\n",
    "\n",
    "                # If the local search returns any solutions, regardless being feasible and/or unfeasibles, they will improve Cmax and so we will add them to the dicts.\n",
    "                feasible_sol_dict.update(neighbour_feasible_dict)\n",
    "                unfeasible_sol_dict.update(neighbour_unfeasible_dict)\n",
    "                search_sol_counter_cum += search_sol_counter\n",
    "\n",
    "                if len(unfeasible_sol_dict) >= 1:\n",
    "\n",
    "                    best_u_Cmax = min(unfeasible_sol_dict.keys())\n",
    "\n",
    "                    if best_u_Cmax < best_grasp_u_Cmax:\n",
    "                        best_grasp_u_Cmax = best_u_Cmax\n",
    "                        best_grasp_u_sol = unfeasible_sol_dict.get(best_u_Cmax)\n",
    "                        best_grasp_u_dict.clear() # Comment this line if the nuimber of unfeasible solutions should be reduced to one\n",
    "                        best_grasp_u_dict.update({best_grasp_u_Cmax: best_grasp_u_sol})\n",
    "                        best_grasp_prec_ok = neighbour_prec_ok\n",
    "                        best_grasp_res_ok = neighbour_res_ok\n",
    "                        grasp_improvement = True\n",
    "\n",
    "                if len(feasible_sol_dict) >= 1:\n",
    "\n",
    "                    best_f_Cmax = min(feasible_sol_dict.keys())\n",
    "\n",
    "                    if best_f_Cmax < best_grasp_f_Cmax:\n",
    "                        best_grasp_f_Cmax = best_f_Cmax\n",
    "                        best_grasp_f_sol = feasible_sol_dict.get(best_f_Cmax)\n",
    "                        best_grasp_f_dict.clear() # Comment this line if the nuimber of feasible solutions should be reduced to one\n",
    "                        best_grasp_f_dict.update({best_grasp_f_Cmax: best_grasp_f_sol})\n",
    "                        best_sol_counter_cum = construct_sol_counter_cum + search_sol_counter_cum\n",
    "                        best_end_time = time.monotonic()\n",
    "                        best_time = (best_end_time - best_start_time)\n",
    "                        grasp_improvement = True\n",
    "\n",
    "                if not grasp_improvement:\n",
    "                    grasp_stagnation += 1\n",
    "                else:\n",
    "                    grasp_stagnation = 0\n",
    "                    grasp_improvement = False\n",
    "\n",
    "                search_end_time = time.monotonic()\n",
    "                search_time += (search_end_time - search_start_time)\n",
    "\n",
    "                grasp_iter_end_time = time.monotonic()\n",
    "                grasp_time += search_time\n",
    "\n",
    "            ########################################################### REPAIR ###########################################################\n",
    "\n",
    "            if (len(best_grasp_f_dict) > 0) and (best_grasp_f_Cmax < np.inf):\n",
    "\n",
    "                instance_Cmax = best_grasp_f_Cmax\n",
    "                best_grasp_sol_dict.update({best_grasp_f_Cmax: best_grasp_f_dict.get(best_grasp_f_Cmax)})\n",
    "                best_sol_prec_ok = True\n",
    "                best_sol_res_ok = True\n",
    "                best_sol_feasibility = best_sol_prec_ok and best_sol_res_ok\n",
    "                repair_needed = False\n",
    "\n",
    "            elif len(best_grasp_u_dict) > 0:\n",
    "\n",
    "                repair_start_time = time.monotonic()\n",
    "                best_grasp_r_dict, best_sol_prec_ok, best_sol_res_ok = repair(best_grasp_u_dict, best_grasp_prec_ok, best_grasp_res_ok, iter_T)\n",
    "                best_sol_feasibility = best_sol_prec_ok and best_sol_res_ok\n",
    "                repair_end_time = time.monotonic()\n",
    "                repair_time = (repair_end_time - repair_start_time)\n",
    "                if best_sol_feasibility:\n",
    "                    best_sol_counter_cum = construct_sol_counter_cum + search_sol_counter_cum\n",
    "                    best_end_time = time.monotonic()\n",
    "                    best_time = (best_end_time - best_start_time)\n",
    "\n",
    "                best_grasp_r_Cmax = list(best_grasp_r_dict.keys())[0]\n",
    "                instance_Cmax = best_grasp_r_Cmax\n",
    "                repair_needed = True\n",
    "                best_grasp_sol_dict.update({best_grasp_r_Cmax: best_grasp_r_dict.get(best_grasp_r_Cmax)})\n",
    "            \n",
    "            print(f'\\nBest solution: Cmax: {instance_Cmax}, feasibility: {best_sol_feasibility} (precedence compliance: {best_sol_prec_ok}, and resource compliance {best_sol_res_ok})')\n",
    "            grasp_sol_counter_cum = construct_sol_counter_cum + search_sol_counter_cum + repair_sol_counter_cum\n",
    "\n",
    "            instance_sol.update({instance_name : [problem_alpha, \n",
    "                                                  instance_T, \n",
    "                                                  T_factor, \n",
    "                                                  instance_Cmax, \n",
    "                                                  best_sol_feasibility, \n",
    "                                                  repair_needed, \n",
    "                                                  round(best_time, 4), \n",
    "                                                  best_sol_counter_cum, \n",
    "                                                  grasp_sol_counter_cum, \n",
    "                                                  grasp_iter, \n",
    "                                                  round(construct_time, 4), \n",
    "                                                  round(search_time, 4), \n",
    "                                                  round(repair_time, 4)]})\n",
    "            \n",
    "            ################################################### GENERATE SOLUTION ###################################################\n",
    "\n",
    "            instance_sol_df = generate_solution(best_grasp_sol_dict.get(instance_Cmax))\n",
    "            sol_name = instance_name + '_sol_T' + str(T_index) + '_a' + str(alpha_index) + '.csv'\n",
    "            sol_file_path = os.path.join(output_path, sol_name)\n",
    "            instance_sol_df.to_csv(sol_file_path, sep=';', header=True, index=True, mode='w', decimal=',')\n",
    "            display(instance_sol_df)\n",
    "            \n",
    "            ################################################### FINAL EVALUATION ###################################################\n",
    "\n",
    "\n",
    "        print('\\n\\n' + ' SUMMARY '.center(139, '#')) #483\n",
    "\n",
    "        column_names = ['Alpha', 'T_limit', 'T_factor', 'Makespan', 'Feasibility', 'Repair', 'Time2best',\n",
    "                        'Sol2best', 'Total_sol', 'GRASP iter', 'Construct_time', 'Search_time', 'Repair_time']\n",
    "        summary_df = pd.DataFrame.from_dict(instance_sol, orient='index', dtype=None, columns=column_names)\n",
    "\n",
    "        display(summary_df)\n",
    "\n",
    "        output_name = instance_path.replace(parent_path, '')\n",
    "        output_name = output_name.lstrip('\\ ')\n",
    "        output_name = output_name + '_T' + str(T_index) + '_a' + str(alpha_index) + '.csv'\n",
    "\n",
    "        output_file_path = os.path.join(output_path, output_name)\n",
    "        summary_df.to_csv(output_file_path, sep=';', header=True, index=True, mode='w', decimal=',')\n",
    "\n",
    "        os.chdir(instance_path)\n",
    "    \n",
    "os.chdir(parent_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
